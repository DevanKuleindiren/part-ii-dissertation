% The document class
\documentclass[a4paper, 12pt]{article}

% The geometry of the pages
\usepackage[a4paper, margin=25mm]{geometry}

% For maths
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathpartir}

% For algorithms
\usepackage{algpseudocode}

% For lists
\usepackage{enumitem}
\usepackage{listings}

% For trees
\usepackage{qtree}

% For hyperlinks
\usepackage{hyperref}

% For formatting columns in a table
\usepackage{array}

% For graphics and colour
\usepackage{graphicx}
\usepackage{color}

% For temporarily splitting the document into multiple columns
\usepackage{multicol}

% For circuit/logic diagrams
\usepackage{circuitikz}

% Enables spacing in file paths when inserting images
\usepackage[space]{grffile}

% Remove paragraph indentation
\setlength\parindent{0pt}

% The colours used by lstlisting
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

% For blocks of code
\lstset{ %
  aboveskip=1em,
  backgroundcolor=\color{white},	% choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\ttfamily\scriptsize,		% the size of the fonts that are used for the code
  belowskip=-2em,
  breakatwhitespace=false,		% sets if automatic breaks should only happen at whitespace
  breaklines=true,				% sets automatic line breaking
  captionpos=b,					% sets the caption-position to bottom
  commentstyle=\color{mygreen},	% comment style
  deletekeywords={...},			% if you want to delete keywords from the given language
  escapeinside={\%*}{*)},			% if you want to add LaTeX within your code
  extendedchars=true,			% lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,					% adds a frame around the code
  keepspaces=true,				% keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},		% keyword style
  language=Java,				% the language of the code
  otherkeywords={},				% if you want to add more keywords to the set, e.g. for ML you might have {fun, let, ...}
  numbers=left,					% where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,				% how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray},	% the style that is used for the line-numbers
  rulecolor=\color{black},			% if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,				% show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,			% underline spaces within strings only
  showtabs=false,				% show tabs within strings adding particular underscores
  stepnumber=1,				% the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},		% string literal style
  tabsize=2,					% sets default tabsize to 2 spaces
  title=\lstname					% show the filename of files included with \lstinputlisting; also try caption instead of title
}

% Defines \ip{arg1}{arg2} to mean (arg1, arg2).
\newcommand{\ip}[2]{(#1, #2)}

% Horizontal rule command used for title
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

% TO DO macro
\newcommand{\todo}{\textbf{\textit{\textcolor{red}{TODO: }}}}

% Shorten textbf
\newcommand{\tbf}[1]{\textbf{#1}}

% Shorten texttt
\newcommand{\ttt}[1]{\texttt{#1}}

% Shorten textit
\newcommand{\tit}[1]{\textit{#1}}

% Blue, underlined hyperlinks
\newcommand{\hlink}[2]{{\href{#1}{#2}}}

% Bullet point
\newcommand{\bpt}[0]{\textbullet~}

% White bullet point
\newcommand{\wbpt}[0]{$\circ$~}

% \cmark and \xmark can be used as a tick and a cross respectively
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\begin{document}

\begin{titlepage}
	\noindent
	\begin{minipage}[t][][t]{0.5\textwidth}
		\includegraphics[width=40mm]{./Images/CamLogo.jpg}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
	\begin{flushright}
		\large
		\textit{Devan Kuleindiren}
		\\
		\textit{Robinson College}
		\\
		\texttt{dk503}
	\end{flushright}
	\end{minipage}
	
	\begin{center}
	\vspace{6cm}
	{\scshape\large Computer Science Tripos - Part II Project Proposal\par}
	\vspace{0.5cm}
	{\huge\bfseries Language Modelling for Text Prediction\par}
	\vspace{0.5cm}
	{\large \today \par}
	\end{center}
	
	\vfill
	
	\begin{center}
	supervised by \\
	Dr. Marek Rei \& Dr. Ekaterina Shutova
	\end{center}
	
	\vspace{1.5cm}
\end{titlepage}

\section*{Introduction}
Language models (LMs) produce a probability distribution over a sequence of words, which can be used to estimate the relative likelihood of words or phrases occurring in various contexts. The predictive power of such models is useful in speech recognition, text prediction, machine translation, handwriting recognition, part-of-speech tagging and information retrieval. For instance, in a text prediction context, if a user has typed:

\begin{center}
	\ttt{`Do you want to grab a '}
\end{center}

then a language model could be used to suggest probable next words such as \ttt{`coffee'}, \ttt{`drink'} or \ttt{`bite'}, and these predictions could further be narrowed down when the user starts typing the next word. \\

With the recent shift from desktop to mobile computing, it has become increasingly important to facilitate accurate, high-speed language models that aren't too memory hungry. The aim of this project is to investigate this area by implementing and benchmarking various language models, with an aim of providing useful insights into how their the accuracy, speed and resource consumption compare. \\

I will begin with the traditional $n$-gram language models and their various smoothing techniques, before proceeding onto neural network-based language models. Combinations of such models, which can be achieved by interpolating their probability distributions, will also be included in the benchmark. \\

In the context of text prediction, one issue with existing language models is that they assume that their input is error (such as spelling or grammatical errors) free. With the presence of typos, human error and people learning new languages, this assumption is often not so reliable. I aim to include some extensions to existing language models that seek to provide better predictions over error-prone text, and I will assess whether they offer any performance improvements by including them in the benchmark.

\section*{Starting point}

\subsection*{Code}
TensorFlow, an open-source machine learning library, will assist in the construction of the neural network-based language models.

\subsection*{Computer Science Tripos}

The following Tripos courses will be useful when undertaking my project:
\begin{itemize}
\item
	\tbf{Natural Language Processing} - $n$-gram language models.
\item
	\tbf{Artificial Intelligence I} - neural networks.
\item
	\tbf{Machine Learning and Bayesian Inference} - tips and tricks around supervised learning.
\item
	\tbf{Mathematical Methods for Computer Science} - Markov chains.
\item
	\tbf{Information Theory} - entropy.
\end{itemize}
\tbf{Algorithms}, \tbf{Object-Oriented Programming}, \tbf{Software and Interface Design}, and \tbf{Software Engineering} will also prove useful for general programming and software engineering. \\

There are aspects of language modelling not covered by the Tripos, including various smoothing techniques for $n$-gram LMs and recurrent neural networks. I will fill this gap with personal reading.

\subsection*{Experience}
Language modelling draws ideas from natural language processing and machine learning. I have no prior experience in the former field, but in the latter I have an understanding of neural networks (including RNNs) that comes from both the Artificial Intelligence I course and personal reading. \\

\section*{Resources required}

\subsection*{Machines}
The development of my project will be carried out on my personal laptop, a 2015 MacBook Pro running macOS Sierra. I accept full responsibility for this machine and I have made contingency plans to protect myself against hardware and/or software failure. \\

I intend to train the language models on my laptop. When experimenting with the neural network-based language models on larger datasets, I may utilise the University's High Performance Computing Service\footnote{\hlink{https://www.cl.cam.ac.uk/local/sys/resources/hpc/}{https://www.cl.cam.ac.uk/local/sys/resources/hpc/}} in order to run the models on a GPU cluster.

\subsection*{Datasets}
In order to train and evaluate the various language models, I will need to make use of various datasets, including, but not limited to:
\begin{itemize}
\item
	The Penn Treebank (PTB) dataset.\footnote{\hlink{http://www.cis.upenn.edu/~treebank/}{http://www.cis.upenn.edu/$\sim$treebank/}}
\item
	The One Billion Word dataset.\footnote{\hlink{http://www.statmt.org/lm-benchmark/}{http://www.statmt.org/lm-benchmark/}}
\item
	The CLC FCE dataset.\footnote{\hlink{http://ilexir.co.uk/applications/clc-fce-dataset/}{http://ilexir.co.uk/applications/clc-fce-dataset/}}
\end{itemize}

\subsection*{Version Control \& Backup}
I will use Git for version control, with one Git repository for all project documentation and another for all project source code. Both repositories will be hosted on GitHub, and all local commits will be pushed no more than a few hours after they are created, meaning that I should never lose more than half a day's worth of work. \\

I will also make weekly backups on an external hard drive, so that in the unlikely event that both GitHub goes down and my laptop fails, I will be able to recover my project on a new machine.

\section*{Work to be done}

\subsection*{Language models}
The first stage of the project is to implement a variety of language modelling algorithms. Starting with the simplest, I will implement $n$-gram models with varying values of $n$. I will also investigate the effect of add-1 smoothing \cite{add1}, Katz smoothing \cite{katz}, absolute discounting \cite{absdisc}, Kneser-Ney smoothing \cite{kneser} and modified Kneser-Ney smoothing \cite{modkneser}. \\

Once the $n$-gram models along with their corresponding smoothing techniques have been implemented, I will implement some neural network-based language models, which have recently proven very effective. I will start with an RNN (recurrent neural network) based language model \cite{mikolov}, and then I will advance to the more complex LSTM (long short-term memory) architecture \cite{lstm}. \\

Combinations of the neural network and $n$-gram based language models, and extensions to the aforementioned models that aim to improve performance over error-prone text, will also be implemented. \\

In order to implement the neural network-based language models, I will make use of TensorFlow, Google's open-source machine learning library. TensorFlow has both Python and C++ APIs, but the Python API is more complete and easier to use\footnote{\hlink{https://www.tensorflow.org/versions/r0.11/api\_docs/cc/index.html}{https://www.tensorflow.org/versions/r0.11/api\_docs/cc/index.html}}, and so I will construct my language models in Python.

\subsection*{Benchmarking framework}
Once a series of language models have been implemented, the next stage would be to implement a framework into which each language model can be inserted. The framework should run a series of tests and generate evaluation for:
\begin{itemize}
\item
	\tbf{Accuracy:} Perplexity, cross-entropy and guessing entropy.
\item
	\tbf{Speed and resource usage:} Time and memory usage at training and inference.
\end{itemize}
Given that I will be evaluating the language models in the context of text prediction, I will also include some metrics for how useful the predictions are for users, such as the number of letters the user could avoid typing as the result of a correct prediction. 

\subsection*{Demonstration application}
In order to demonstrate the language models in a practical context, I aim to integrate them into a simple console application that predicts the next word in the user's sentence.

\section*{Success criteria}

This project will be deemed successful if the following criteria are met:

\begin{itemize}
\item
	Language models (LMs) using the following techniques are implemented:
	\begin{itemize}
	\item
		$N$-gram LMs with various smoothing techniques.
	\item
		A vanilla RNN-based LM.
	\item
		An LSTM-based LM.
	\end{itemize}
\item
	Comprehensible and reliable comparisons between the various LM implementations and their combinations are made regarding their accuracy, speed and resource consumption during both training and inference.
\item
	A simple console application is developed to demonstrate the capability of the aforementioned language models in the context of next-word prediction.
\end{itemize}

\section*{Possible extensions}

\subsection*{Mobile Keyboard}
One possible extension would be to integrate the language model implementations into a system-wide mobile keyboard on iOS - made possible as of iOS 8.\footnote{\scriptsize{\hlink{https://developer.apple.com/library/content/documentation/General/Conceptual/ExtensibilityPG/CustomKeyboard.html}{https://developer.apple.com/library/content/documentation/General/Conceptual/ExtensibilityPG/CustomKeyboard.html}}} This would involve constructing a simple keyboard interface which displays suggestions for the next word in the user's sentence, by querying the language model(s) and filtering the results as the next word is typed. TensorFlow supports exportation of models into a language-independent format (using Google's protocol buffers), which means I could train and export my language models in Python and then load them into iOS using TensorFlow's C++ API.

\newpage
\section*{Timetable}

\subsection*{Michaelmas term}
\begin{tabular}{l | l}
	06/10 - 19/10 & \bpt Refine project idea and write project proposal. \\
	& \bpt Background reading on $n$-gram language models and smoothing \\
	&~~~techniques. \\
	\tbf{Milestone} & \tit{Project proposal finished.} \\ \hline
	20/10 - 26/10 & \bpt Implement a simple bigram language model, without any smoothing. \\
	& \bpt Background reading on recurrent neural networks and long short-term \\
	&~~~memory in particular. \\
	\tbf{Milestone} & \tit{A working implementation of a basic bigram language model completed.} \\ \hline
	27/10 - 09/11 & \bpt Implement a test framework for generating evaluation metrics for a \\
	&~~~given language model. \\
	\tbf{Milestone} & \tit{Test framework complete.} \\ \hline
	10/11 - 16/11 & \bpt Implement the various smoothing techniques outlined in `Work to \\
	&~~~be done'. \\
	\tbf{Milestone} & \tit{A working implementation of $n$-gram language models with various} \\
	& \tit{smoothing techniques completed.} \\ \hline
	17/11 - 23/11 & \bpt Implement vanilla RNN language model. \\
	\tbf{Milestone} & \tit{A working implementation of RNN-based language model.} \\ \hline
	24/11 - 30/11 & \bpt Implement LSTM-based language model. \\
	\tbf{Milestone} & \tit{A working implementation of LSTM-based language model.} \\ \hline
\end{tabular}

\subsection*{Christmas vacation}
\begin{tabular}{l | l}
	01/12 - 14/12 & \bpt Experiment with extending the existing language models to improve \\
	&~~~performance on error-prone text. \\ \hline
	05/01 - 18/01 & \bpt Benchmark and evaluate the various language models. \\
	& \bpt Start implementing an application to demonstrate the capability of the \\
	&~~~language models in the context of next-word prediction. \\
	\tbf{Milestone} & \tit{Language models benchmarked.} \\
\end{tabular}

\subsection*{Lent term}
\begin{tabular}{l | l}
	19/01 - 01/02 & \bpt Write progress report. \\
	& \bpt Complete demonstration application. \\
	\tbf{Milestone} & \tit{Progress report and demonstration application complete.} \\ \hline
	02/02 - 22/02 & \bpt Rigorous testing and bug-fixing of project. \\
	& \bpt If time allows, implement project extensions. \\
	& \bpt Present project to overseers. \\
	\tbf{Milestone} & \tit{Project presentation given and implementation complete.} \\ \hline
	23/02 - 15/03 & \bpt Start writing dissertation. \\
\end{tabular}

\subsection*{Easter vacation}
\begin{tabular}{l | l}
	16/03 - 19/04 & \bpt Finish writing dissertation and proof-read it. \\
	\tbf{Milestone} & \tit{Final dissertation draft submitted to supervisor and Director of Studies.} \\
\end{tabular}

\begin{center}
\horrule{1pt}
\tit{Submission deadline on 19/05/2017.} \\[-5pt]
\horrule{1pt}
\end{center}

\begin{thebibliography}{99}

\bibitem{add1}
	W.E. Johnson. Probability: deductive and inductive problems
	\tit{Mind}, Vol. 41, No. 164, pp. 409-423, 1932.
	
\bibitem{katz}
	S.M. Katz. Estimation of probabilities from sparse data for the language model component of a speech recognizer. \tit{IEEE Transactions on Acoustics, Speech and Signal Processing}, Vol. 35, No. 3, pp. 400-401, 1987.
	
\bibitem{absdisc}
	H. Ney, U. Essen and R. Kneser. On structuring probabilistic dependences in stochastic language modeling. \tit{Computer, Speech, and Language}, Vol. 8, No. 1, pp. 1-38, 1994.   
	
\bibitem{kneser}
	R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In \tit{Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing}, Vol. 1, pp. 181-184, 1995.

\bibitem{modkneser}
	S.F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. \tit{Computer Speech \& Language}, Vol. 13, No. 4, pp. 359-394, 1999.

\bibitem{mikolov}
	T. Mikolov, M. Karafi\'{a}t, L. Burget, J. Cernock\'{y}, and S. Khudanpur. Recurrent neural network based language model. In \tit{Interspeech}, Vol. 2, pp. 3, 2010.
	
\bibitem{lstm}
	S. Hochreiter and J. Schmidhuber. Long short-term memory. \tit{Neural Computation}, Vol. 9, No. 8, pp. 1735-1780, 1997.

\end{thebibliography}

\end{document}
