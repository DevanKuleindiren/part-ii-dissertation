% The document class
\documentclass[a4paper, 12pt]{report}

% The geometry of the pages
\usepackage[a4paper, margin=25mm]{geometry}

% For maths
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathpartir}

% For algorithms
\usepackage{algpseudocode}

% For lists
\usepackage{enumitem}
\usepackage{listings}

% For trees
\usepackage{qtree}

% For hyperlinks
\usepackage{hyperref}

% For formatting columns in a table
\usepackage{array}

% For graphics and colour
\usepackage{graphicx}
\usepackage{color}

% For temporarily splitting the document into multiple columns
\usepackage{multicol}

% For circuit/logic diagrams
\usepackage{circuitikz}

% Enables spacing in file paths when inserting images
\usepackage[space]{grffile}

% Remove paragraph indentation
\setlength\parindent{0pt}

% The colours used by lstlisting
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

% For blocks of code
\lstset{ %
  aboveskip=1em,
  backgroundcolor=\color{white},	% choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\ttfamily\scriptsize,		% the size of the fonts that are used for the code
  belowskip=-2em,
  breakatwhitespace=false,		% sets if automatic breaks should only happen at whitespace
  breaklines=true,				% sets automatic line breaking
  captionpos=b,					% sets the caption-position to bottom
  commentstyle=\color{mygreen},	% comment style
  deletekeywords={...},			% if you want to delete keywords from the given language
  escapeinside={\%*}{*)},			% if you want to add LaTeX within your code
  extendedchars=true,			% lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,					% adds a frame around the code
  keepspaces=true,				% keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},		% keyword style
  language=Java,				% the language of the code
  otherkeywords={},				% if you want to add more keywords to the set, e.g. for ML you might have {fun, let, ...}
  numbers=left,					% where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,				% how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray},	% the style that is used for the line-numbers
  rulecolor=\color{black},			% if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,				% show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,			% underline spaces within strings only
  showtabs=false,				% show tabs within strings adding particular underscores
  stepnumber=1,				% the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},		% string literal style
  tabsize=2,					% sets default tabsize to 2 spaces
  title=\lstname					% show the filename of files included with \lstinputlisting; also try caption instead of title
}

% Defines \ip{arg1}{arg2} to mean (arg1, arg2).
\newcommand{\ip}[2]{(#1, #2)}

% Horizontal rule command used for title
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

% TO DO macro
\newcommand{\todo}{\textbf{\textit{\textcolor{red}{TODO: }}}}

% Shorten textbf
\newcommand{\tbf}[1]{\textbf{#1}}

% Shorten texttt
\newcommand{\ttt}[1]{\texttt{#1}}

% Shorten textit
\newcommand{\tit}[1]{\textit{#1}}

% Blue, underlined hyperlinks
\newcommand{\hlink}[2]{{\href{#1}{#2}}}

% Bullet point
\newcommand{\bpt}[0]{\textbullet~}

% White bullet point
\newcommand{\wbpt}[0]{$\circ$~}

% \cmark and \xmark can be used as a tick and a cross respectively
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

% Set the Table of Contents depth.
\setcounter{tocdepth}{2}

\begin{document}

\begin{titlepage}
	\noindent
	\begin{minipage}[t][][t]{0.5\textwidth}
		\includegraphics[width=40mm]{./Images/CamLogo.jpg}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
	\begin{flushright}
		\large
		\textit{Devan Kuleindiren}
		\\
		\textit{Robinson College}
		\\
		\texttt{dk503}
	\end{flushright}
	\end{minipage}
	
	\begin{center}
	\vspace{6cm}
	{\scshape\large Computer Science Tripos - Part II Project\par}
	\vspace{0.5cm}
	{\huge\bfseries Language Modelling for Text Prediction\par}
	\vspace{0.5cm}
	{\large \today \par}
	\end{center}
	
	\vfill
	
	\begin{center}
	supervised by \\
	Dr Marek Rei \& Dr Ekaterina Shutova
	\end{center}
	
	\vspace{1.5cm}
\end{titlepage}

% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Devan Kuleindiren \\
College:            & \bf Robinson College \\
Project Title:      & \bf Language Modelling for Text Prediction \\
Examination:        & \bf Computer Science Tripos -- Part II, June 2017 \\
Word Count:         & \bf ? \\
Project Originator: & Devan Kuleindiren \& Dr Marek Rei \\
Supervisors:         & Dr Marek Rei \& Dr Ekaterina Shutova \\
\end{tabular}
}
\stepcounter{footnote}

\section*{Original Aims of the Project}

\section*{Work Completed}

\section*{Special Difficulties}

 
\newpage
\section*{Declaration}

I, Devan Kuleindiren of Robinson College, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed:}

\medskip
\leftline{Date:}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}
\section{Motivation}
\section{Related Work}

\chapter{Preparation}
\section{N-Gram Models}
\subsection{An Overview of N-Gram Models}
Describe how they work, and the motivation for smoothing and backoff.
\subsection{Smoothing Techniques}
\subsubsection{Add-One Smoothing}
\subsubsection{Katz Smoothing}
\subsubsection{Absolute Discounting}
\subsubsection{Kneser-Ney}
\subsubsection{Modified Kneser-Ney}
\section{Recurrent Neural Network Models}
\subsection{An Overview of Neural Networks}
Give a brief introduction to neural networks. Explain backpropagation. Motivate why we need RNNs for language modelling.
\subsection{Recurrent Neural Networks}
Explain RNNs.
\subsubsection{Vanilla Recurrent Neural Networks}
\subsubsection{Gated Recurrent Unit}
\subsubsection{Long Short-Term Memory}
\subsection{Word Embeddings}
\subsection{Backpropagation Through Time}
\section{Software Engineering}
\subsection{Starting Point}
\subsection{Requirements}
\subsection{Tools and Technologies Used}

\chapter{Implementation}
\section{Development Strategy}
\subsection{Version Control and Build Tools}
\subsection{Testing Strategy}
\section{System Overview}
\subsection{Interface to Language Models}
\section{N-Gram Models}
\subsection{Counting N-Grams Efficiently}
\subsection{Precomputing Smoothing Coefficients}
\section{Recurrent Neural Network Models}
\subsection{The Forward Pass}
\subsection{Gradient Updates}
\subsection{Network Architectures}
\subsection{Parameter Tuning}
\subsection{The Balance Between Underfitting and Overfitting}
Dropout, embedding, learning rate decay, momentum?, gradient clipping
\section{Extending Models to Tackle Error-Prone Text}
\subsection{Preprocessing the CLC Dataset}
\subsection{Error Correction on Word Context}
\section{Benchmarking Framework}
\subsection{Metrics for Accuracy}
\subsection{Metrics for Resource Consumption}
\section{Mobile Keyboard}
\subsection{Updating Language Model Predictions On the Fly}

\chapter{Evaluation}
\tit{`Accuracy' could be any one of perplexity, average-keys-saved or guessing entropy.}
\section{Evaluation Methodology}
\section{Results}
\subsection{Existing Models}
Things I aim to evaluate here are: \\
\tbf{N-Gram Models:}
\begin{itemize}
\item
	Accuracy as a function of the amount of training data used (one line per LM), for various smoothing techniques \tit{(on subset of 1BN word dataset)}.
\item
	The effect of increasing N.
\end{itemize}

\tbf{Neural Models}
\begin{itemize}
\item
	Accuracy as a function of the amount of training data used (one line per LM), for various RNN architectures \tit{(on subset of 1BN word dataset)}.
\item
	The effect of changing the number of hidden neurons.
\end{itemize}

\tbf{All Models}
\begin{itemize}
\item
	A comparison of the performance of all models \tit{(on the PTB dataset)}.
\end{itemize}

\tbf{Combinations of Models}
\begin{itemize}
\item
	How combinations of models compare with respect to standalone ones \tit{(on the PTB dataset)}.
\end{itemize}


\subsection{On a Mobile Device}
Here, I want to focus on the tradeoff between accuracy and resource consumption. Specifically, I could look at the following: \\
\begin{itemize}
\item
	The effect of increasing the minimum frequency for a word to be considered in the vocabulary. (I.e. the effect of changing the vocabulary size).
\item
	The effect of changing the number of hidden layer neurons.
\item
	The effect of using RNN vs GRU vs LSTM.
\item
	(Perhaps also the effect of pruning on n-gram models).
\end{itemize}

\subsection{On Error-Prone Text}
Things I aim to evaluate here are: \\
\begin{itemize}
\item
	The hypothetical upper and lower bounds on accuracy (i.e. the LM results on correct input and on incorrect input respectively).
\item
	The effect of using the vocabulary vs different sized dictionaries for determining if a word should be replaced or not.
\item
	The effect of edit distance on performance.
\item
	An intuitive explanation behind the gap remaining between the current performance and the upper bound. Perhaps some suggestions for future work.
\end{itemize}

\chapter{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\end{document}
